{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT3.5 + MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_chat.storage.mongo import mongodb_client, mongodb_uri\n",
    "from llama_index import SummaryIndex\n",
    "from llama_index.readers import SimpleMongoReader\n",
    "import openai\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"products\"\n",
    "collection_name = \"data_xs\"\n",
    "query_dict = {} # passed into db.collection.find()\n",
    "field_names = [\"product_name\"]\n",
    "reader = SimpleMongoReader(uri=mongodb_uri)\n",
    "documents = reader.load_data(\n",
    "    db_name, collection_name, field_names, query_dict=query_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = SummaryIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Do you have any light bulbs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Source: https://docs.llamaindex.ai/en/latest/module_guides/loading/connector/root.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"products\"\n",
    "COLLECTION_NAME = \"data\"\n",
    "FIELD_NAMES = [\"product_url\", \"product_name\", \"description\"]\n",
    "SEPARATOR = \" \\n\"\n",
    "QUERY_DICT = {\"description\": { \"$type\": \"string\" }} # Will be passed into db.collection.find() -> TODO: need to check data types to prevent errors or clean the data\n",
    "MAX_DOCS = 50\n",
    "METADATA_NAMES = [\"sale_price\", \"brand\", \"category\", \"available\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseReader: https://github.com/run-llama/llama_index/blob/08caf8119c66b4dfb5899da0efce09bb2c62bf96/llama_index/readers/base.py#L24\n",
    "# SimpleMongoReader: https://github.com/run-llama/llama_index/blob/08caf8119c66b4dfb5899da0efce09bb2c62bf96/llama_index/readers/mongo.py\n",
    "reader = SimpleMongoReader(uri=mongodb_uri)\n",
    "documents = reader.load_data(\n",
    "    DB_NAME, \n",
    "    COLLECTION_NAME, \n",
    "    FIELD_NAMES, \n",
    "    separator = SEPARATOR, \n",
    "    query_dict=QUERY_DICT,\n",
    "    max_docs = MAX_DOCS,\n",
    "    metadata_names = METADATA_NAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='73d35005-ca59-48ae-97ed-c2101114edf3', embedding=None, metadata={'sale_price': 31.93, 'brand': 'La Costeï¿½ï¿½a', 'category': 'Food | Meal Solutions, Grains & Pasta | Canned Goods | Canned Vegetables', 'available': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='c689b341afa6f28007625c316c7f15a89f51519df8b8e3b2ecb2bfc0b9f07618', text='https://www.walmart.com/ip/La-Costena-Chipotle-Peppers-7-OZ-Pack-of-12/139941530 \\nLa Costena Chipotle Peppers, 7 OZ (Pack of 12) \\n We aim to show you accurate product information. Manufacturers, suppliers and others provide what you see here, and we have not verified it. See our disclaimer |La Costena Chipotle Peppers, 7 OZ (Pack of 12) Easy open. Ready to serve! Product of Mexico.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ed53629-fe11-4a07-b6e7-9bd896351eb6', embedding=None, metadata={'sale_price': 10.48, 'brand': 'Equate', 'category': 'Health | Equate | Equate Allergy | Equate Sinus Congestion & Nasal Care', 'available': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8b399d01353bd546d5d7721b13ea20fabfdc033ab5255dbd62c1cae71a18544a', text='https://www.walmart.com/ip/Equate-Triamcinolone-Acetonide-Nasal-Allergy-Spray-55-mcg-per-spray-0-37-fl-oz/632775553 \\nEquate Triamcinolone Acetonide Nasal Allergy Spray, 55 mcg per spray, 0.37 fl oz \\n We aim to show you accurate product information. Manufacturers, suppliers and others provide what you see here, and we have not verified it. See our disclaimer |Compare to Nasacort Allergy 24HR active ingredient. Triamcinolone Acetonide Nasal Allergy Spray, 55 mcg per spray, allergy symptom reliever (glucocorticoid)*, is a non-drowsy, multi-symptom nasal spray for 24 hour relief of allergy symptoms such as nasal congestion, sneezing, runny and itchy nose. For adults and children ages 2 years and older. *Triamcinolone acetonide is a steroid medicine known as a glucocorticoid. Equate Multi-Symptom Nasal Allergy Spray, 60 Sprays, 0.37 fl oz: ACTIVE INGREDIENT: This multi-symptom nasal allergy spray provides 24 hour relief in adults and children ages 2 years and older. Compare to the active ingredient in Nasacort Allergy 24HR. EFFECTIVE: This nasal spray temporarily relieves symptoms of hay fever or other upper respiratory allergies: nasal congestion, runny nose, sneezing and itchy nose. 24 HOUR RELIEF: This nasal allergy spray provides 24 hours of relief from allergy symptoms. Use this nasal spray to help relieve allergy symptoms like nasal congestion, sneezing, runny nose and itchy nose. NON-DROWSY: This multi-symptom nasal allergy spray provides all day, non-drowsy relief. ALLERGY TIPS: If you have allergies, total avoidance is one of the best strategies. Unfortunately, many allergens are not easy to eliminate. Use a multi-symptom nasal spray like Triamcinolone Acetonide Nasal Allergy Spray, 55 mcg per spray can help. |ACTIVE INGREDIENT: This multi-symptom nasal allergy spray provides 24 hour relief in adults and children ages 2 years and older. Compare to the active ingredient in Nasacort Allergy 24HR.|EFFECTIVE: This nasal spray temporarily relieves symptoms of hay fever or other upper respiratory allergies: nasal congestion, runny nose, sneezing and itchy nose.|24 HOUR RELIEF: This nasal allergy spray provides 24 hours of relief from allergy symptoms. Use this nasal spray to help relieve allergy symptoms like nasal congestion, sneezing, runny nose and itchy nose.|NON-DROWSY: This multi-symptom nasal allergy spray provides all day, non-drowsy relief.|ALLERGY TIPS: If you have allergies, total avoidance is one of the best strategies. Unfortunately, many allergens are not easy to eliminate. Use a multi-symptom nasal spray like Triamcinolone Acetonide Nasal Allergy Spray, 55 mcg per spray can help.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69bd90b3-7fa1-4f72-af81-562cb7e61634', embedding=None, metadata={'sale_price': 17.99, 'brand': 'Twinings', 'category': 'Food | Beverages | Tea | All Tea', 'available': False}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='30707f4ddd190146929bb98221fbafd6df44fe57b1d25cc18a2259e38e04e0d4', text='https://www.walmart.com/ip/6-Boxes-Twinings-of-London-Nightly-Calm-Green-Tea-Bags-20-Ct/55450414 \\n(6 Boxes) Twinings of London Nightly Calm Green Tea Bags, 20 Ct \\n We aim to show you accurate product information. Manufacturers, suppliers and others provide what you see here, and we have not verified it. See our disclaimer |Enojy one of your favorite tea flavors with this set from Twinings. This set of 120 tea bags comes in the Twinings Nightly Calm bagged green tea flavor. Naturally decaffeinated, you can enjoy a cup of tea before bed and not worry about being kept awake at night. With these traditional style tea bags, all you have to do is steep them in hot water and enjoy.20 bags Twinings of London Nightly Calm Green Tea Bags, 20 Count, Pack of 6 A comforting herbal tea made with soothing camomile, cooling spearmint and the subtle flavor of lemongrass. Steep for three to five minutes for the perfect cup of nightly calm tea. |Twinings of London Nightly Calm Green Tea Bags, 20 Count, Pack of 6|A comforting herbal tea made with soothing camomile, cooling spearmint and the subtle flavor of lemongrass. Steep for three to five minutes for the perfect cup of nightly calm tea.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e31be5f-6c3d-40ca-a1c8-6faca7d0ab68', embedding=None, metadata={'sale_price': 13.76, 'brand': 'DJO', 'category': 'Health | Diabetes Care | Diabetic Socks', 'available': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2e75b4553e7692628ca94c4596e1b0b311fda8fbfbc77e60e82578ac0c13b9d1', text='https://www.walmart.com/ip/Diabetic-Socks-Small-White-Item-Number-11600SPR-Small-1-Pair-Pair/156849799 \\nDiabetic Socks Small White - Item Number 11600SPR - Small - 1 Pair / Pair \\n We aim to show you accurate product information. Manufacturers, suppliers and others provide what you see here, and we have not verified it. See our disclaimer |Diabetic Socks Small White - Item Number 11600SPR - Small - 1 Pair / Pair Manufacturer # 11600S Manufacturer DJO Application Diabetic Socks Color White Material Cotton Size Choose: Small / Medium / Large UNSPSC Code 42142802 User Unisex Features Assure a smooth fit and reduce the risk of irritation and development of sores Perfect for dress shoes or custom molded shoes. A flat-knit sock with full heel and toe Non-restrictive, non-ribbed top that will not hinder circulation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96fa49dd-d9de-4289-930a-741cce491a13', embedding=None, metadata={'sale_price': 79.99, 'brand': 'Fuel Pureformance', 'category': 'Health | Sports Medicine & Injury Recovery Solution | Optimize Performance | Train For A Specific Event | Boxing', 'available': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0d0ff02263b629aab2371d10d2855493605b12d6c937704f8da1e503144b8e7e', text='https://www.walmart.com/ip/FUEL-Pureformance-Heavy-Bag-Stand-Black/55505439 \\nFUEL Pureformance Heavy Bag Stand, Black \\n We aim to show you accurate product information. Manufacturers, suppliers and others provide what you see here, and we have not verified it. See our disclaimer |The FUEL Pureformance Heavy Bag Stand is the perfect stand for hanging your punching bag and upping your boxing and MMA game. Standing just over seven feet tall, the FUEL Pureformance stand holds your heavy bag in the optimum position for both hand and foot work. With the convenience of hanging a heavy bag at home or the office, you can get a quick cardio and strength workout, relieve some stress or just get some blood pumping easily without having to go anywhere. A good boxing workout uses most of your muscle groups from your legs to your arms, strengthening your core and improving your balance. So, whether you take time to get a session in or take a few short blasts at the bag here and there throughout the day, you will feel the benefits in no time. The FUEL Pureformance Heavy Bag Stand is made from heavy-duty steel tubing with a scratch-resistant powder-coated finish, making it tough enough to take a beating. FUEL Pureformance Heavy Bag Stand, Black: Made from heavy-duty steel tubing Scratch-resistant powder-coated finish Weight pegs with foam stops for increased stability (weights not included) 2 bottom tube hooks for optional bag stabilization (strap not included) Maximum heavy bag weight: 100 lbs (bag not included) Dimensions: 55.51\"W x 47.64\"D x 87.4\"H |Made from heavy-duty steel tubing|Scratch-resistant powder-coated finish|Weight pegs with foam stops for increased stability (weights not included)|2 bottom tube hooks for optional bag stabilization (strap not included)|Maximum heavy bag weight: 100 lbs (bag not included)|Dimensions: 55.51\"W x 47.64\"D x 87.4\"H', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "NOT TODO: We might need to extend or redo the SimpleMongoReader to add information on the `relationships`, maybe by linking similar products (by category or brand, or other).\n",
    "\n",
    "This might not be necessary, as relationships denote informaiton on the order of the Nodes or the Parent/Child nodes, with Documents seems as if only the SOURCE can be specified as a relationship (https://github.com/search?q=repo%3Arun-llama%2Fllama_index+DocumentRelationship&type=code). therefore maybe we don't need relationships in this case, and categories can be added as a list in the metadata.\n",
    "\n",
    "Relationships can be set like this (as per Node info https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes.html):\n",
    "```\n",
    "node[0].relationships[NodeRelationship.NEXT] = RelatedNodeInfo(\n",
    "    node_id=node[1].node_id\n",
    ")\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: we could even set the id of the document to the URL, for example by setting `documents[0].id_`.\n",
    "\n",
    "Also, we might want to add info in metadata for better retrieval which is not added to the prompt: information relavant to rank the information but not relevant for the response: https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents.html#customizing-llm-metadata-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what will be feed to the LLM for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sale_price: 31.93\n",
      "brand: La Costeï¿½ï¿½a\n",
      "category: Food | Meal Solutions, Grains & Pasta | Canned Goods | Canned Vegetables\n",
      "available: True\n",
      "\n",
      "https://www.walmart.com/ip/La-Costena-Chipotle-Peppers-7-OZ-Pack-of-12/139941530 \n",
      "La Costena Chipotle Peppers, 7 OZ (Pack of 12) \n",
      " We aim to show you accurate product information. Manufacturers, suppliers and others provide what you see here, and we have not verified it. See our disclaimer |La Costena Chipotle Peppers, 7 OZ (Pack of 12) Easy open. Ready to serve! Product of Mexico.\n",
      "------------\n",
      "sale_price: 31.93\n",
      "brand: La Costeï¿½ï¿½a\n",
      "category: Food | Meal Solutions, Grains & Pasta | Canned Goods | Canned Vegetables\n",
      "available: True\n",
      "\n",
      "https://www.walmart.com/ip/La-Costena-Chipotle-Peppers-7-OZ-Pack-of-12/139941530 \n",
      "La Costena Chipotle Peppers, 7 OZ (Pack of 12) \n",
      " We aim to show you accurate product information. Manufacturers, suppliers and others provide what you see here, and we have not verified it. See our disclaimer |La Costena Chipotle Peppers, 7 OZ (Pack of 12) Easy open. Ready to serve! Product of Mexico.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "print(documents[0].get_content(metadata_mode=MetadataMode.LLM)) # What the LLM model will see when crafting the response\n",
    "print(\"------------\")\n",
    "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED)) # What the embedding model will see when ranking the information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE: I might have to rewrite a custom Reader in order to load the data from MongoDB, for example, I'd like some fields to be both in the FIELD_NAMES and metadata parts, but in the text I can only have strings, therefore I might need to make it custom so the fields that are not strings get transformed when added to the FIELD_NAMES. Also personalizing a bit more the FIELD_NAMES field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nodes (check)\n",
    "\n",
    "Source: https://docs.llamaindex.ai/en/latest/module_guides/loading/documents_and_nodes/root.html#\n",
    "\n",
    "- Document: container arround a data source (PDF, API, data from DB).\n",
    "- Node: represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.\n",
    "\n",
    "We might not need to separate Documents into Nodes, as informaiton is not that big on a document, but rather, try to have one Node per document? Or, if possible, only use the documents directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might want to transform the Documents into Nodes and do certain transformations by using an Ingestion Pipeline. We can also use it to store the embeddings into a Vector Store.\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html \n",
    "# Here are the possible transformations: https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations.html\n",
    "# For starters, we might want to use the SimpleFileNodeParser, and later on see if we can improve the Node parsing or only use the Documents: https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: Do not run!!\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.extractors import TitleExtractor\n",
    "from llama_index.ingestion import IngestionPipeline, IngestionCache\n",
    "import asyncio\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(), # Creating the embedding is to create the Index\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=documents) \n",
    "# NOTE: This fails on Jupyter Notebook due to an already asyncio.run() process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error about Notebooks can be avoided by runing this:\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "# Because it fails with pipeline, we run separate transformations:\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20) # If we increase the chunk_size we can end up with a Node per Document.\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation is something that takes a list of nodes as an input, and returns a list of nodes. Currently, the following components are Transformation objects:\n",
    "- [TextSplitter](ttps://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#text-splitters): In our case the only one that should work is SentenceSplitter, which attempts to split text while respecting the boundaries of sentences.\n",
    "- [NodeParser](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html): create nodes based on the type of content that is being parsed. Not suited for our case.\n",
    "- [MetadataExtractor](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor.html): For now it might not be necessary, but **TODO: we can test some of these in the future**!!\n",
    "    - `SummaryExtractor` - automatically extracts a summary over a set of Nodes.\n",
    "    - `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer.\n",
    "    - `TitleExtractor` - extracts a title over the context of each Node.\n",
    "    - `EntityExtractor` - extracts entities (i.e. names of places, people, things) mentioned in the content of each Node.\n",
    "- [Embeddings model](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-embeddings): Various integrations supported, **TODO: This will need to be changed depending on the model beeing used!!**\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: Do not run!!\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "# client = QdrantClient(path=\"path/to/db\") # -> Path to Docker for persistent storage\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Ingest directly into a vector db\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "# It also supports async operations\n",
    "# nodes = await pipeline.arun(documents=documents)\n",
    "\n",
    "# Create your index\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above example, embeddings are calculated as part of the pipeline. If you are connecting your pipeline to a vector store, embeddings must be a stage of your pipeline or your later instantiation of the index will fail.\n",
    "\n",
    "You can omit embeddings from your pipeline if you are not connecting to a vector store, i.e. just producing a list of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Index\n",
    "\n",
    "https://docs.llamaindex.ai/en/latest/understanding/indexing/indexing.html#\n",
    "\n",
    "An Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\n",
    "\n",
    "A Vector Store Index turns all of your text into embeddings using an API from your LLM, this is what is meant when we say it “embeds your text”. If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls. When you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\n",
    "\n",
    "Once the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as k, so the parameter controlling how many embeddings to return is known as top_k. This whole type of search is often referred to as “top-k semantic retrieval” for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = SummaryIndex.from_documents(documents)\n",
    "# This can be used with documents, but if we want to use embeddings for faster search, we need Nodes, as the Nodes have a fixed chunk size, I think...\n",
    "# Source: https://docs.llamaindex.ai/en/latest/understanding/indexing/indexing.html#vector-store-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 50/50 [00:00<00:00, 1130.38it/s]\n",
      "Generating embeddings:   0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 52/52 [00:01<00:00, 40.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index_from_docs = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even as though we have 50 documents, the `from_documents` created 52 embeddings, this is because when you use from_documents, your Documents are split into chunks and parsed into Node objects. We could control the Node creation settings by adding the ServiceContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 50/50 [00:00<00:00, 975.97it/s]\n",
      "Generating embeddings:   0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 63/63 [00:01<00:00, 50.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "service_context_custom = ServiceContext.from_defaults(text_splitter=text_splitter)\n",
    "\n",
    "index_from_docs_custom = VectorStoreIndex.from_documents(documents, service_context = service_context_custom, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that since we set the SentenceSplitter method (which is a NodeParser) we now have 63 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x14cfeb340>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_from_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index_from_nodes = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Storing\n",
    "\n",
    "Check best database for vector store: https://docs.llamaindex.ai/en/latest/module_guides/storing/vector_stores.html#\n",
    "\n",
    "Qdrant allos to store the vectors and also the metadata, here called Payload: https://qdrant.tech/documentation/concepts/payload/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running Qdrant on Docker, we can access the UI, where we can visualize the vectors, see that the default configuration is set to cosine distance and the size of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/latest/examples/vector_stores/QdrantIndexDemo.html#\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "qdrant = QdrantClient(location=\":memory:\") # Create in-memory Qdrant instance, for testing, CI/CD\n",
    "# OR\n",
    "# client = QdrantClient(\"localhost\", port=6333)  # Persists changes to disk, fast prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults()\n",
    "vector_store = QdrantVectorStore(client=qdrant, collection_name=\"products\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can build the vector store but asyncronously: https://docs.llamaindex.ai/en/latest/examples/vector_stores/QdrantIndexDemo.html#build-the-vectorstoreindex-asynchronously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Querying\n",
    "\n",
    "https://docs.llamaindex.ai/en/latest/understanding/querying/querying.html#\n",
    "\n",
    "Querying consists of three distinct stages:\n",
    "1. **Retrieval** is when you find and return the most relevant documents for your query from your Index. As previously discussed in indexing, the most common type of retrieval is “top-k” semantic retrieval, but there are many other retrieval strategies.\n",
    "2. **Postprocessing** is when the Nodes retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached or require that the retrieved nodes reach a minimum similarity score to be included.\n",
    "3. **Response synthesis** is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# THIS IS THE VANILLA IMPLEMENTATION\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are the cheepest products?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The cheapest products are not mentioned in the given context information.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do the query async: https://docs.llamaindex.ai/en/latest/examples/vector_stores/QdrantIndexDemo.html#async-query-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the ganular implementation, where we can select each of the steps:\n",
    "\n",
    "#### Retriever\n",
    "\n",
    "TODO: we need to look into this for further customization: https://docs.llamaindex.ai/en/latest/module_guides/querying/retriever/root.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing & Response synthesis\n",
    "\n",
    "##### Postprocessing:\n",
    "- Node postprocessors can provide filters and augmentation that can improve the relevancy of the retrieved Node objects and help reduce the time/number of LLM calls/cost.\n",
    "- Here's a list of all Node Postprocessors: https://docs.llamaindex.ai/en/latest/api_reference/node_postprocessor.html# (each `pydantic model` is a postprocessor).\n",
    "\n",
    "##### Response Synthesizer:\n",
    "Here are the options supported: https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes.html ; REFINE seems like the best option for us, but we need to check what \"create and refine\" means in the documentation. We can also consider creating our own synthesizer: https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#custom-response-synthesizers\n",
    "\n",
    "Then, we can even ensure a structured response with Pydantic: https://docs.llamaindex.ai/en/latest/module_guides/querying/structured_outputs/query_engine.html# \n",
    "This could be useful to extract a structure for analysis puerpouses and provide links, prices, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.response_synthesizers import ResponseMode\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    # response_mode=ResponseMode.REFINE -> REFINE is not working very good, the default is best for now\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "We have a variety of light bulbs available, including the following options:\n",
      "- 03102 Light Bulb, Vanity Globe, Clear, 720 Lumens, 60-Watts - Quantity 1\n",
      "- AduroSmart ERIA Soft White Smart A19 Light Bulb CRI 90+, 60W Equivalent, Hub Required\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What light bulbs do you have?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with the Pydantic pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# NOTE: This structure seems to be too complicated!!\n",
    "\n",
    "class Product(BaseModel):\n",
    "    \"\"\"Data model for a product\"\"\"\n",
    "    name: str\n",
    "    product_url: str\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Data model for a response.\"\"\"\n",
    "    products: List[Product]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    \"\"\"Data model for a response.\"\"\"\n",
    "    products: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    # response_mode=ResponseMode.REFINE -> REFINE is not working very good, the default is best for now\n",
    "    output_cls = Response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"products\":[\"03102 Light Bulb, Vanity Globe, Clear, 720 Lumens, 60-Watts - Quantity 1\",\"AduroSmart ERIA Soft White Smart A19 Light Bulb CRI 90+, 60W Equivalent, Hub Required\"]}\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What light bulbs do you have?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompts\n",
    "\n",
    "- First check this: https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/usage_pattern.html\n",
    "- https://docs.llamaindex.ai/en/latest/module_guides/models/prompts.html# \n",
    "- https://docs.llamaindex.ai/en/stable/examples/customization/prompts/chat_prompts.html\n",
    "- Investigate LangChain: I think that LangChain should be used to identify when RAG is needed and maybe to add ReAct on top, but the RAG prompting can be made with Llama-Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating\n",
    "\n",
    "https://docs.llamaindex.ai/en/latest/understanding/evaluating/evaluating.html\n",
    "\n",
    "here are some open-source tools (i.e., DeepEval) that help us debug and evaluate the results: https://docs.llamaindex.ai/en/latest/module_guides/observability/observability.html#deepeval\n",
    "\n",
    "DeepEval has different metrics: latency, hallucination, faithfulness, summarization, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. All toguether\n",
    "\n",
    "https://docs.llamaindex.ai/en/latest/understanding/putting_it_all_together/putting_it_all_together.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
